{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ac1f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6623a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.BertWordPieceTokenizer(strip_accents=False)\n",
    "\n",
    "#no training file yet still looking\n",
    "\n",
    "tokenizer.train('Train.txt', vocab_size=5000)\n",
    "\n",
    "tokenizer.save_model('tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function singh ji \n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"/content/Tokenizer\", do_lower_case=False)\n",
    "\n",
    "\n",
    "#dont have any file to train on yet so is a place holder\n",
    "def get_tokenized_dataset():\n",
    "  data_files = {\"train\": \"/content/Train.txt\"}\n",
    "  tokenized_datasets = datasets.load_dataset('text', data_files=data_files)\n",
    "    \n",
    "    \n",
    "def tokenize_function(examples):\n",
    "    # Remove the empty lines\n",
    "    examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"do_not_pad\",\n",
    "        truncation=\"do_not_truncate\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2156bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig, DistilBertForMaskedLM\n",
    "\n",
    "config = DistilBertConfig(vocab_size=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_collator():\n",
    "  return DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e4462",
   "metadata": {},
   "outputs": [],
   "source": [
    " args = TrainingArguments(output_dir=\"/content/Checkpoints\", do_train=True, per_device_train_batch_size=32,weight_decay=0.01, \n",
    "                    num_train_epochs=3, save_total_limit=2, save_steps=500,\n",
    "                    disable_tqdm=False, remove_unused_columns=False, ignore_data_skip=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "       model=model,\n",
    "       args=args,\n",
    "       train_dataset=tokenized_datasets[\"train\"],\n",
    "       tokenizer=tokenizer,\n",
    "       data_collator=data_collator,\n",
    "   )\n",
    "   trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
